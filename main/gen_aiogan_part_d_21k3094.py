# -*- coding: utf-8 -*-
"""gen-aiogan-part-d-21k3094.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jrP_IRJG2bVZphZInmDp_X2Chb-n_U0l
"""

import os
from xml.etree import ElementTree as ET
from PIL import Image

VOC_ROOT = '/kaggle/input/gen-ai-dataset'
IMAGE_DIR = os.path.join(VOC_ROOT, '/kaggle/input/gen-ai-dataset/VOCtrainval/VOCdevkit/VOC2008/JPEGImages')
ANNOTATION_DIR = os.path.join(VOC_ROOT, '/kaggle/input/gen-ai-dataset/VOCtrainval/VOCdevkit/VOC2008/Annotations')
TRAIN_LIST = os.path.join(VOC_ROOT, '/kaggle/input/gen-ai-dataset/VOCtrainval/VOCdevkit/VOC2008/ImageSets/Main/train.txt')

def get_train_image_paths_and_labels():
    with open(TRAIN_LIST) as f:
        image_ids = [x.strip() for x in f.readlines()]

    class_to_images = {}

    for img_id in image_ids:
        annotation_path = os.path.join(ANNOTATION_DIR, f"{img_id}.xml")
        tree = ET.parse(annotation_path)
        root = tree.getroot()
        classes = [obj.find("name").text for obj in root.findall("object")]

        for cls in classes:
            if cls not in class_to_images:
                class_to_images[cls] = []
            class_to_images[cls].append(os.path.join(IMAGE_DIR, f"{img_id}.jpg"))

    return class_to_images

class_to_images = get_train_image_paths_and_labels()
print(f"Found {len(class_to_images)} classes")

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

image_size = 64
z_dim = 100
num_classes = len(class_to_images)

transform = transforms.Compose([
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

class ClassImageDataset(Dataset):
    def __init__(self, image_paths, transform):
        self.image_paths = image_paths
        self.transform = transform

    def __getitem__(self, index):
        img_path = self.image_paths[index]
        image = Image.open(img_path).convert("RGB")
        return self.transform(image)

    def __len__(self):
        return len(self.image_paths)

class Generator(nn.Module):
    def __init__(self, z_dim, class_dim, img_channels=3):
        super().__init__()
        self.label_embed = nn.Embedding(class_dim, z_dim)
        self.model = nn.Sequential(
            nn.Linear(z_dim * 2, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, img_channels * image_size * image_size),
            nn.Tanh()
        )

    def forward(self, noise, labels):
        label_embedding = self.label_embed(labels)
        x = torch.cat([noise, label_embedding], dim=1)
        img = self.model(x)
        img = img.view(img.size(0), 3, image_size, image_size)
        return img

class Discriminator(nn.Module):
    def __init__(self, class_dim, img_channels=3):
        super().__init__()
        self.label_embed = nn.Embedding(class_dim, image_size * image_size)
        self.model = nn.Sequential(
            nn.Linear(img_channels * image_size * image_size + image_size * image_size, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, img, labels):
        img_flat = img.view(img.size(0), -1)
        label_embedding = self.label_embed(labels)
        x = torch.cat([img_flat, label_embedding], dim=1)
        return self.model(x)

import torch.optim as optim

def train_aiogan_for_class(class_name, image_paths, epochs=100, batch_size=32):
    dataset = ClassImageDataset(image_paths, transform)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    G = Generator(z_dim, num_classes).to(device)
    D = Discriminator(num_classes).to(device)
    criterion = nn.BCELoss()
    optimizer_G = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))

    class_idx = list(class_to_images.keys()).index(class_name)
    print(f"Training AIOGAN for class: {class_name} with {len(image_paths)} images")

    for epoch in range(epochs):
        for real_imgs in loader:
            batch_size = real_imgs.size(0)
            real_imgs = real_imgs.to(device)

            real_labels = torch.ones(batch_size, 1).to(device)
            fake_labels = torch.zeros(batch_size, 1).to(device)

            class_labels = torch.full((batch_size,), class_idx, dtype=torch.long).to(device)

            # Train Discriminator
            noise = torch.randn(batch_size, z_dim).to(device)
            fake_imgs = G(noise, class_labels)
            D_real = D(real_imgs, class_labels)
            D_fake = D(fake_imgs.detach(), class_labels)

            loss_D = criterion(D_real, real_labels) + criterion(D_fake, fake_labels)
            optimizer_D.zero_grad()
            loss_D.backward()
            optimizer_D.step()

            # Train Generator
            fake_imgs = G(noise, class_labels)
            D_fake = D(fake_imgs, class_labels)
            loss_G = criterion(D_fake, real_labels)

            optimizer_G.zero_grad()
            loss_G.backward()
            optimizer_G.step()

        print(f"[{epoch+1}/{epochs}] Loss D: {loss_D.item():.4f}, Loss G: {loss_G.item():.4f}")

    return G

import torchvision.utils as vutils

import os
from PIL import Image
import torchvision.transforms as transforms

def generate_synthetic_samples(G, class_name, num_samples=10):
    G.eval()
    noise = torch.randn(num_samples, z_dim).to(device)
    class_idx = list(class_to_images.keys()).index(class_name)
    class_labels = torch.full((num_samples,), class_idx, dtype=torch.long).to(device)

    base_dir = os.path.join("VOCaugmented", "VOCdevkit", "VOC2008")
    jpeg_dir = os.path.join(base_dir, "JPEGImages")
    imageset_dir = os.path.join(base_dir, "ImageSets", "Main")

    os.makedirs(jpeg_dir, exist_ok=True)
    os.makedirs(imageset_dir, exist_ok=True)

    transform = transforms.ToPILImage()

    file_lines = []

    with torch.no_grad():
        fake_imgs = G(noise, class_labels)
        for i, img in enumerate(fake_imgs):
            img_name = f"{class_name}_aug_{i+1:04d}.jpg"
            img_path = os.path.join(jpeg_dir, img_name)

            pil_img = transform(img.cpu())
            pil_img.save(img_path)

            file_lines.append(f"{os.path.splitext(img_name)[0]} 1\n")

    # Save ImageSets/Main/class_aug.txt file
    with open(os.path.join(imageset_dir, f"{class_name}_aug.txt"), 'w') as f:
        f.writelines(file_lines)

    print(f"Saved {num_samples} images for class '{class_name}' to JPEGImages/, list in ImageSets/Main/{class_name}_aug.txt")

for class_name, image_paths in class_to_images.items():
    if len(image_paths) < 20:
        print(f"Skipping '{class_name}' â€” only {len(image_paths)} samples available")
        continue

    print(f"\n--- Starting augmentation for class: {class_name} ---")

    # Train AIOGAN using only training data for this class
    G = train_aiogan_for_class(class_name, image_paths, epochs=50, batch_size=32)

    generate_synthetic_samples(G, class_name, num_samples=100)

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score
import warnings
warnings.filterwarnings("ignore")

DATA_DIR = '/kaggle/working/VOCaugmented'
JPEG_IMAGES_DIR = os.path.join(DATA_DIR, '/kaggle/working/VOCaugmented/VOCdevkit/VOC2008/JPEGImages')
IMAGESETS_MAIN_DIR = os.path.join(DATA_DIR, '/kaggle/working/VOCaugmented/VOCdevkit/VOC2008/ImageSets', '/kaggle/working/VOCaugmented/VOCdevkit/VOC2008/ImageSets/Main')

VOC_CLASSES = [
    'aeroplane_aug', 'bicycle', 'bird', 'boat', 'bottle',
    'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person',
    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

BATCH_SIZE = 16
IMG_SIZE = (224, 224)
NUM_EPOCHS = 30
LEARNING_RATE = 0.001

def build_labels_dict(split='train'):
    """
    Build a dictionary mapping each image ID to a multi-label vector of length 20.
    It reads per-class label files (e.g., aeroplane_train.txt) from IMAGESETS_MAIN_DIR.
    """
    labels_dict = {}
    for class_idx, cls in enumerate(VOC_CLASSES):
        file_path = os.path.join(IMAGESETS_MAIN_DIR, f"{cls}_{split}.txt")
        with open(file_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                image_id = parts[0]
                # +1 indicates presence; any other value is considered absent.
                label = 1 if int(parts[1]) == 1 else 0
                if image_id not in labels_dict:
                    labels_dict[image_id] = np.zeros(len(VOC_CLASSES), dtype=np.float32)
                labels_dict[image_id][class_idx] = label
    return labels_dict

def get_image_ids_and_labels(split='train'):
    """
    Returns a sorted list of image IDs and a corresponding numpy array of labels.
    """
    labels_dict = build_labels_dict(split=split)
    image_ids = sorted(list(labels_dict.keys()))
    labels = [labels_dict[img_id] for img_id in image_ids]
    return image_ids, np.array(labels)

def get_combined_image_ids_and_labels():
    """
    Combines the augmented label files (like 'aeroplane_aug.txt', ...) into one label dictionary.
    Returns:
        combined_ids: list of image IDs
        combined_labels: numpy array of shape [num_images, 20]
    """
    labels_dict = {}

    for class_idx, cls in enumerate(VOC_CLASSES):
        file_path = os.path.join(IMAGESETS_MAIN_DIR, f"{cls}_aug.txt")
        try:
            with open(file_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    img_id = parts[0]
                    label = int(parts[1])
                    if img_id not in labels_dict:
                        labels_dict[img_id] = np.zeros(len(VOC_CLASSES), dtype=np.float32)
                    if label == 1:
                        labels_dict[img_id][class_idx] = 1.0
        except FileNotFoundError:
            print(f"Warning: File not found {file_path}, skipping...")

    combined_ids = sorted(list(labels_dict.keys()))
    combined_labels = np.array([labels_dict[img_id] for img_id in combined_ids], dtype=np.float32)

    return combined_ids, combined_labels

combined_ids, combined_labels = get_combined_image_ids_and_labels()
total_images = len(combined_ids)
print("Total combined images:", total_images)

import shutil, zipfile

export_folder = '/kagle/working/combined_images'
os.makedirs(export_folder, exist_ok=True)
for img_id in combined_ids:
    src = os.path.join(JPEG_IMAGES_DIR, f"{img_id}.jpg")
    dst = os.path.join(export_folder, f"{img_id}.jpg")
    shutil.copy(src, dst)

zip_path = '/kagle/working/combined_images.zip'
with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
    for fname in os.listdir(export_folder):
        zf.write(os.path.join(export_folder, fname), arcname=fname)

print(f"All {total_images} images exported and zipped at {zip_path}")

indices = np.arange(total_images)
np.random.shuffle(indices)
split_index = int(0.8 * total_images)
train_indices = indices[:split_index]
test_indices  = indices[split_index:]

train_ids = [combined_ids[i] for i in train_indices]
train_labels = combined_labels[train_indices]
test_ids  = [combined_ids[i] for i in test_indices]
test_labels = combined_labels[test_indices]

print(f"Number of images in new training dataset: {len(train_ids)}")
print(f"Number of images in new testing dataset: {len(test_ids)}")

TEST_IDS = test_ids

def load_and_preprocess_image(image_path, augment=False):
    """
    Loads an image, decodes it, resizes to IMG_SIZE, and applies ImageNet normalization.
    Optionally applies random horizontal flipping for augmentation.
    """
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, IMG_SIZE)
    if augment:
        image = tf.image.random_flip_left_right(image)
    image = tf.cast(image, tf.float32) / 255.0
    mean = tf.constant([0.485, 0.456, 0.406])
    std  = tf.constant([0.229, 0.224, 0.225])
    image = (image - mean) / std
    return image

def create_dataset(image_ids, labels, augment=False):
    """
    Creates a tf.data.Dataset from given image IDs and labels.
    The image file paths are constructed from the JPEG_IMAGES_DIR.
    """
    image_paths = [os.path.join(JPEG_IMAGES_DIR, f"{img_id}.jpg") for img_id in image_ids]
    labels_tensor = tf.convert_to_tensor(labels, dtype=tf.float32)
    image_paths = tf.constant(image_paths, dtype=tf.string)
    image_ids_tensor = tf.constant(image_ids, dtype=tf.string)

    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels_tensor, image_ids_tensor))

    def _process(image_path, label, image_id):
        image = load_and_preprocess_image(image_path, augment)
        return image, label, image_id

    dataset = dataset.map(_process, num_parallel_calls=tf.data.AUTOTUNE)
    if augment:
        dataset = dataset.shuffle(buffer_size=1000)
    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    return dataset

train_dataset = create_dataset(train_ids, train_labels, augment=True).map(lambda image, label, id: (image, label))
test_dataset  = create_dataset(test_ids, test_labels, augment=False).map(lambda image, label, id: (image, label))

train_image_count = sum(1 for _ in train_dataset) * BATCH_SIZE
test_image_count  = sum(1 for _ in test_dataset) * BATCH_SIZE
print(f"Total images in training dataset: {train_image_count}")
print(f"Total images in testing dataset: {test_image_count}")

def get_pretrained_model_tf(model_name, num_classes=20):
    """
    Returns a compiled tf.keras model for multi-label classification.
    Loads a pretrained base and adds a Dense layer with sigmoid activation.
    For each architecture, local weights files are manually loaded.
    """
    input_tensor = layers.Input(shape=(*IMG_SIZE, 3))

    if model_name.lower() == 'vgg16':
        local_weights_path = '/kaggle/input/vgg16-weights-tf-dim-ordering-tf-kernels-notop-h5/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'
        base_model = tf.keras.applications.VGG16(
            include_top=False,
            weights=None,
            input_tensor=input_tensor,
            pooling='avg'
        )
        base_model.load_weights(local_weights_path)
        base_model.trainable = False

    elif model_name.lower() == 'resnet50':
        base_model = tf.keras.applications.ResNet50(
            include_top=False,
            weights='imagenet',
            input_tensor=input_tensor,
            pooling='avg'
        )
        base_model.trainable = True

        x = base_model.output
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.5)(x)
        outputs = layers.Dense(num_classes, activation='sigmoid')(x)

        model = models.Model(inputs=base_model.input, outputs=outputs)

        model.compile(
            optimizer=optimizers.Adam(learning_rate=1e-5),
            loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05),
            metrics=[]
        )
        return model

    elif model_name.lower() == 'densenet121':
        base_model = tf.keras.applications.DenseNet121(
            include_top=False,
            weights='imagenet',
            input_tensor=input_tensor,
            pooling='avg'
        )
        base_model.trainable = True

    elif model_name.lower() == 'mobilenet_v2':
        local_weights_path = '/kaggle/input/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5/tensorflow2/mobilenet_v2/1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'
        base_model = tf.keras.applications.MobileNetV2(
            include_top=False,
            weights=None,
            input_tensor=input_tensor,
            pooling='avg'
        )
        base_model.load_weights(local_weights_path)
        base_model.trainable = False

    else:
        raise ValueError(f"Model {model_name} not supported.")

    x = base_model.output
    outputs = layers.Dense(num_classes, activation='sigmoid')(x)
    model = models.Model(inputs=base_model.input, outputs=outputs)

    model.compile(
        optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=[]
    )
    return model

def compute_mAP(y_true, y_scores):
    """
    Computes a simplified mean Average Precision (mAP) for multi-label classification.
    For each class, it calculates the precision at positions where a positive label occurs,
    and then averages these values over all classes.
    """
    num_classes = y_true.shape[1]
    APs = []
    for c in range(num_classes):
        scores_c = y_scores[:, c]
        labels_c = y_true[:, c]
        sorted_indices = np.argsort(-scores_c)
        labels_sorted = labels_c[sorted_indices]
        total_positives = np.sum(labels_c)
        if total_positives == 0:
            APs.append(0)
            continue
        tp = 0
        precision_scores = []
        for i, label in enumerate(labels_sorted):
            if label == 1:
                tp += 1
                precision_scores.append(tp / (i + 1))
        APs.append(np.mean(precision_scores) if precision_scores else 0)
    return np.mean(APs)

def get_predictions(model, dataset):
    """
    Obtains predictions and ground truth labels from the dataset.
    Returns:
      y_true: numpy array of shape [num_images, 20]
      y_scores: numpy array of shape [num_images, 20]
    """
    all_labels = []
    all_predictions = []
    for batch in dataset:
        images, labels = batch
        preds = model.predict(images)
        all_labels.append(labels.numpy())
        all_predictions.append(preds)
    return (np.concatenate(all_labels, axis=0),
            np.concatenate(all_predictions, axis=0))

def get_top_k_images(model, dataset, k=10):
    """
    Computes predictions for the entire testing dataset and uses the global TEST_IDS (which are in the same order)
    to return a list of tuples: (image_id, average_score) for the top k images.
    """
    predictions = model.predict(dataset)
    avg_scores = np.mean(predictions, axis=1)
    sorted_indices = np.argsort(-avg_scores)
    top_k = [(TEST_IDS[i], avg_scores[i]) for i in sorted_indices[:k]]
    return top_k

def compute_multilabel_metrics(y_true, y_scores, threshold=0.5):
    """
    Given:
      y_true   : np.array of shape [N_samples, N_classes], 0/1 groundâ€‘truth
      y_scores : np.array of shape [N_samples, N_classes], modelâ€™s sigmoid outputs
    Returns:
      accuracy, precision, recall (all computed microâ€‘averaged over all labels)
    """
    # 1) Binarize predictions
    y_pred = (y_scores >= threshold).astype(int)

    # 2) Flatten to compute microâ€‘averaged metrics across all (sample, class) pairs
    y_true_flat = y_true.flatten()
    y_pred_flat = y_pred.flatten()

    acc  = accuracy_score(y_true_flat, y_pred_flat)
    prec = precision_score(y_true_flat, y_pred_flat, zero_division=0)
    rec  = recall_score(y_true_flat, y_pred_flat, zero_division=0)
    return acc, prec, rec

trained_models = {}


print("\n===== TRAINING VGG16 =====")
model = get_pretrained_model_tf('vgg16', num_classes=20)

history = model.fit(train_dataset,
                    epochs=NUM_EPOCHS,
                    validation_data=test_dataset)

trained_models['vgg16'] = model

y_true, y_preds = get_predictions(model, test_dataset)
mAP_val = compute_mAP(y_true, y_preds)
print(f"VGG16 - Testing mAP: {mAP_val:.4f}")

print("Top-10 images for VGG16:")
top10 = get_top_k_images(model, test_dataset, k=10)
for rank, (img_id, score) in enumerate(top10, start=1):
    print(f"  {rank:2d}. {img_id} (Average Score: {score:.4f})")

save_path = os.path.join('/kaggle/working', "VGG16_voc2008.h5")
model.save(save_path)
print(f"Saved VGG16 model to {save_path}")

callbacks = [
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)
]

print("\n===== TRAINING RESNET50 =====")
resnet50_model = get_pretrained_model_tf('resnet50', num_classes=20)

history_resnet50 = resnet50_model.fit(train_dataset,
                                     epochs=NUM_EPOCHS,
                                     validation_data=test_dataset,
                                     callbacks=callbacks)

trained_models['resnet50'] = resnet50_model

y_true_resnet50, y_preds_resnet50 = get_predictions(resnet50_model, test_dataset)
mAP_resnet50 = compute_mAP(y_true_resnet50, y_preds_resnet50)
print(f"ResNet50 - Testing mAP: {mAP_resnet50:.4f}")

print("Top-10 images for ResNet50:")
top10_resnet50 = get_top_k_images(resnet50_model, test_dataset, k=10)
for rank, (img_id, score) in enumerate(top10_resnet50, start=1):
    print(f"  {rank:2d}. {img_id} (Average Score: {score:.4f})")

resnet50_save_path = os.path.join('/kaggle/working', "ResNet50_voc2008.h5")
resnet50_model.save(resnet50_save_path)
print(f"Saved ResNet50 model to {resnet50_save_path}")

print("\n===== TRAINING DENSENET121 =====")
densenet121_model = get_pretrained_model_tf('densenet121', num_classes=20)

history_densenet121 = densenet121_model.fit(train_dataset,
                                            epochs=NUM_EPOCHS,
                                            validation_data=test_dataset)

trained_models['densenet121'] = densenet121_model

y_true_densenet121, y_preds_densenet121 = get_predictions(densenet121_model, test_dataset)
mAP_densenet121 = compute_mAP(y_true_densenet121, y_preds_densenet121)
print(f"Densenet121 - Testing mAP: {mAP_densenet121:.4f}")

print("Top-10 images for Densenet121:")
top10_densenet121 = get_top_k_images(densenet121_model, test_dataset, k=10)
for rank, (img_id, score) in enumerate(top10_densenet121, start=1):
    print(f"  {rank:2d}. {img_id} (Average Score: {score:.4f})")

densenet121_save_path = os.path.join('/kaggle/working', "Densenet121_voc2008.h5")
densenet121_model.save(densenet121_save_path)
print(f"Saved Densenet121 model to {densenet121_save_path}")

print("\n===== TRAINING MOBILENETV2 =====")
mobilenet_v2_model = get_pretrained_model_tf('mobilenet_v2', num_classes=20)

history_mobilenet_v2 = mobilenet_v2_model.fit(train_dataset,
                                              epochs=NUM_EPOCHS,
                                              validation_data=test_dataset)

trained_models['mobilenet_v2'] = mobilenet_v2_model

y_true_mobilenet_v2, y_preds_mobilenet_v2 = get_predictions(mobilenet_v2_model, test_dataset)
mAP_mobilenet_v2 = compute_mAP(y_true_mobilenet_v2, y_preds_mobilenet_v2)
print(f"MobileNetV2 - Testing mAP: {mAP_mobilenet_v2:.4f}")

print("Top-10 images for MobileNetV2:")
top10_mobilenet_v2 = get_top_k_images(mobilenet_v2_model, test_dataset, k=10)
for rank, (img_id, score) in enumerate(top10_mobilenet_v2, start=1):
    print(f"  {rank:2d}. {img_id} (Average Score: {score:.4f})")

mobilenet_v2_save_path = os.path.join('/kaggle/working', "MobilenetV2_voc2008.h5")
mobilenet_v2_model.save(mobilenet_v2_save_path)
print(f"Saved MobileNetV2 model to {mobilenet_v2_save_path}")

records = []

for sample_size, models in sample_metrics.items():
    for model_name, metrics in models.items():
        records.append({
            'Sample Size': sample_size,
            'Model':       model_name,
            'mAP':         metrics['mAP'],
            'Accuracy':    metrics['Accuracy'],
            'Precision':   metrics['Precision'],
            'Recall':      metrics['Recall']
        })

# Create and format DataFrame
df_metrics = pd.DataFrame(records)
df_metrics = df_metrics[['Sample Size', 'Model', 'mAP', 'Accuracy', 'Precision', 'Recall']]
print(df_metrics.to_markdown(index=False))

# Prepare data for plotting
sample_sizes = sorted(sample_metrics.keys())
models = list(next(iter(sample_metrics.values())).keys())

# Plot mAP for each model
plt.figure(figsize=(10, 6))
for model in models:
    map_values = [sample_metrics[size][model]['mAP'] for size in sample_sizes]
    plt.plot(sample_sizes, map_values, marker='o', label=model)

plt.title("mAP vs Sample Size for Each Model")
plt.xlabel("Sample Size")
plt.ylabel("mAP")
plt.xticks(sample_sizes)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

records = []

for name, model in trained_models.items():
    y_true, y_scores = get_predictions(model, test_dataset)
    y_pred = (y_scores >= 0.5).astype(int)
    y_true_flat = y_true.flatten()
    y_pred_flat = y_pred.flatten()

    acc   = accuracy_score(y_true_flat, y_pred_flat)
    prec  = precision_score(y_true_flat, y_pred_flat, zero_division=0)
    rec   = recall_score(y_true_flat, y_pred_flat, zero_division=0)
    mAP_v = compute_mAP(y_true, y_scores)

    records.append({
        'Model':        name.upper(),
        'mAP':          mAP_v,
        'Accuracy':     acc,
        'Precision':    prec,
        'Recall':       rec
    })

df_metrics = pd.DataFrame(records)
df_metrics = df_metrics[['Model','mAP','Accuracy','Precision','Recall']]
print(df_metrics.to_markdown(index=False))